{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习入门__一元线性回归\n",
    "---\n",
    "pytorch官网<https://pytorch.org/get-started/locally/>\n",
    "安装后测试\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "# 结果：tensor([[0.3380, 0.3845, 0.3217],\n",
    "#              [0.8337, 0.9050, 0.2650],\n",
    "#              [0.2979, 0.7141, 0.9069],\n",
    "#              [0.1449, 0.1132, 0.1375],\n",
    "#              [0.4675, 0.3947, 0.1426]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.术语\n",
    "- 1. 损失函数 \n",
    "---\n",
    "## 1.2 张量（Tensors）\n",
    "-  它相当于Numpy的多维数组，相较于Numpy,Tensors可以应用到GPU上加快计算速度\n",
    "-  使用时需要导入库 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 声明和定义\n",
    "- torch.empty(): 声明一个未初始化的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个 5*3 的矩阵\n",
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.rand()：随机初始化一个矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个随机初始化的 5*3 矩阵\n",
    "rand_x = torch.rand(5, 3)\n",
    "print(rand_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.zeros():创建数值都是0的矩阵\n",
    "- torch.ones(): 创建数值都是1的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个数值皆是 0，类型为 long 的矩阵\n",
    "zero_x=torch.zeros(5,3,dtype=torch.long)\n",
    "print(zero_x)\n",
    "ones_x=torch.ones(5,3,dtype=torch.long)\n",
    "print(ones_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.tesor()：直接传递tensor数值来创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor数值是[5.5,3]\n",
    "tensor1 = torch.tensor([5.5,3])\n",
    "print(tensor1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了上述几种方法，还可以根据已有的 tensor 变量创建新的 tensor 变量，这种做法的好处就是可以保留已有 tensor 的一些属性，包括尺寸大小、数值属性，除非是重新定义这些属性。相应的实现方法如下：\n",
    "\n",
    "- tensor.new_ones()：new_*()方法需要输入尺寸大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示定义新的尺寸是 5*3，数据类型是 torch.double\n",
    "tensor2 =tensor1.new_ones(5,3,dtype=torch.double) #new_* 方法需要输入tensor大小\n",
    "print(tensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.randn_like(old_tensor)：保留相同尺寸大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#改变数值类型\n",
    "tensor3 = torch.randn_like(tensor2,dtype=torch.float)\n",
    "print('tensor3:',tensor3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensor.size()：获取tensors的尺寸大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor3.size())  \n",
    "# 输出: torch.Size([5, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**： torch.Size 实际上是**元组(tuple)**类型，所以支持所有的元组操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2.2 操作**###\n",
    "1. 加法\n",
    "实现方式：\n",
    "- +运算符\n",
    "- **torch.add(tensor1,tensor2,[out=tensor3])**\n",
    "- **tensor1.add_(tensor2)：直接修改tensor变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor4 = torch.rand(5, 3)\n",
    "print('tensor3 + tensor4= ', tensor3 + tensor4)\n",
    "print('tensor3 + tensor4= ', torch.add(tensor3, tensor4))\n",
    "# 新声明一个 tensor 变量保存加法操作的结果\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(tensor3, tensor4, out=result)\n",
    "print('add result= ', result)\n",
    "# 直接修改变量\n",
    "tensor3.add_(tensor4)\n",
    "print('tensor3= ', tensor3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**：可以改变 tensor 变量的操作都带有一个后缀 _, 例如 x.copy_(y), x.t_() 都可以改变 x 变量\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于 Tensor 的访问，和 Numpy 对数组类似，可以使用索引来访问某一维的数据，如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 访问 tensor3 第一列数据\n",
    "print(tensor3[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-torch.view()：修改Tensor的尺寸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "# -1 表示除给定维度外的其余维度的乘积\n",
    "z = x.view(-1, 8)\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensor.item()：对只有一个元素的tensor获取一个类似py中的整数类型的数值\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 **和Numpy数组的转换**\n",
    "Tensor 和 Numpy 的数组可以相互转换，并且两者转换后共享在 CPU 下的内存空间，即改变其中一个的数值，另一个变量也会随之改变。\n",
    "\n",
    "### 1.3.1 **Tensor转换为Numpy数组**\n",
    "-tensor.numpy():Tensor->Numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 **Numpy数组转换为Tensor**\n",
    "- torch.from_numpy(numpy_array)：Numpy->Tensor\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 **CUDA张量**\n",
    "Tensors 可以通过 .to 方法转换到不同的设备上，即 CPU 或者 GPU 上。例子如下所示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#当 CUDA 可用的时候，可用运行下方这段代码，采用 torch.device() 方法来改变 tensors 是否在 GPU 上进行计算操作\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # 定义一个 CUDA 设备对象\n",
    "    y = torch.ones_like(x, device=device)  # 显示创建在 GPU 上的一个 tensor\n",
    "    x = x.to(device)                       # 也可以采用 .to(\"cuda\") \n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # .to() 方法也可以改变数值类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出结果，第一个结果就是在 GPU 上的结果，打印变量的时候会带有 device='cuda:0'，而第二个是在 CPU 上的变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    tensor([1.4549], device='cuda:0')\n",
    "    tensor([1.4549], dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **autograd**\n",
    "库，主要提供对 Tensors 上所有运算操作的自动微分功能，也就是计算梯度的功能。它属于`define-by-run` 类型框架，即反向传播操作的定义是根据代码的运行方式，因此每次迭代都可以是不同的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 **张量**\n",
    "`torch.Tensor` 是 Pytorch 最主要的库，当设置它的属性 `.requires_grad=True`，那么就会开始追踪在该变量上的所有操作，而完成计算后，可以调用 ``.backward()`` 并自动计算所有的梯度，得到的梯度都保存在属性 ``.grad`` 中。\n",
    "调用 ``.detach()`` 方法分离出计算的历史，可以停止一个 tensor 变量继续追踪其历史信息 ，同时也防止未来的计算会被追踪。\n",
    "\n",
    "而如果是希望防止跟踪历史（以及使用内存），可以将代码块放在 with torch.no_grad(): 内，这个做法在使用一个模型进行评估的时候非常有用，因为模型会包含一些带有 ``requires_grad=True`` 的训练参数，但实际上并不需要它们的梯度信息。\n",
    "\n",
    "``Tensor`` 和 `Function` 两个类是有关联并建立了一个非循环的图，可以编码一个完整的计算记录。每个 `tensor` 变量都带有属性 .grad_fn ，该属性引用了创建了这个变量的 Function （除了由用户创建的 Tensors，它们的 grad_fn=None\n",
    "---\n",
    "- backward()：对Tensor求导运算，求梯度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONV：卷积计算层，线性乘积求和。\n",
    "RELU：激励层，ReLU是激活函数的一种。\n",
    "POOL：池化层，简言之，即取区域平均或最大。\n",
    "PCA/白化等等。CNN只对训练集做“去均值”这一步\n",
    "FC：全连接层\n",
    "这几个部分中，卷积计算层是CNN（神经网络）的核心，下文将重点阐述。\n",
    "未知图案的局部和标准X图案的局部一个一个比对时的计算过程，便是卷积操作\n",
    "3.2 什么是卷积\n",
    "对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。\n",
    "a. 深度depth：神经元个数，决定输出的depth厚度，同时代表滤波器个数。\n",
    "b. 步长stride：决定滑动多少步可以到边缘。\n",
    "c. 填充值zero-padding：在外围边缘补充若干圈0，方便从初始位置以步长为单位可以刚好滑倒末尾位置，通俗地讲就是为了总长能被步长整除。\n",
    "\n",
    "左边数据在变化，每次滤波器都是针对某一局部的数据窗口进行卷积，这就是所谓的CNN中的局部感知机制。\n",
    "\n",
    "打个比方，滤波器就像一双眼睛，人类视角有限，一眼望去，只能看到这世界的局部。如果一眼就看到全世界，你会累死，而且一下子接受全世界所有信息，你大脑接收不过来。当然，即便是看局部，针对局部里的信息人类双眼也是有偏重、偏好的。比如看美女，对脸、胸、腿是重点关注，所以这3个输入的权重相对较大。\n",
    "\n",
    "nn.Flatten()作用：将连续的维度范围展平为张量。 经常在nn.Sequential()中出现，一般写在某个神经网络模型之后，用于对神经网络模型的输出进行处理，得到tensor类型的数据。\n",
    "\n",
    "生成器，迭代器\n",
    "#训练mlp\n",
    "Adam优化器\n",
    "mlp_optimizer=torch.optim.Adam(mlp.parameters(), lr=MLP_LR)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(mlp_optimizer, gamma=0.998)\n",
    "lr_history = []\n",
    "mlp_loss=[]\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    lr_history.append(mlp_optimizer.param_groups[0]['lr'])\n",
    "    preds=mlp(input_x)\n",
    "    loss=torch.nn.functional.mse_loss(preds,labels)\n",
    "    mlp_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    mlp_optimizer.step()\n",
    "    scheduler.step()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
